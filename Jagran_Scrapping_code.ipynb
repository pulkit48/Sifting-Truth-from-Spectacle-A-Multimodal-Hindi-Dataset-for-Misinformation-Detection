{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvC5r7to1Kfl",
        "outputId": "089851ef-cfeb-4592-fea6-abe8445a9c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.19.0-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.10.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.19.0 trio-0.25.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20jCQyfK1Qfa",
        "outputId": "c04843f9-4702-47d5-9dbd-c74a607afde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fake_useragent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzgnXkyZ1Qb1",
        "outputId": "e74f4416-a8ed-4376-ac0d-e0b20d20cb37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: fake_useragent\n",
            "Successfully installed fake_useragent-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dz0qJ041QaS",
        "outputId": "70bc2de1-283e-412c-fa03-9d0ccdcc9eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading urls in csv file"
      ],
      "metadata": {
        "id": "FbG_mFlPUJEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import hashlib\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Define the root URL\n",
        "root_url = 'https://www.jagran.com/fact-check/news-news-hindi.html'\n",
        "\n",
        "# Function to extract article URL\n",
        "def extract_article_data(article):\n",
        "    link = article.find('a')['href']\n",
        "    article_url = urljoin(root_url, link)\n",
        "    return {\n",
        "        'URL': article_url\n",
        "    }\n",
        "\n",
        "# Function to scroll to the bottom of the page\n",
        "def scroll_to_bottom(driver):\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    time.sleep(3)  # Reduced sleep duration\n",
        "\n",
        "# Initialize WebDriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument(\"--disable-images\")\n",
        "\n",
        "ua = UserAgent()\n",
        "user_agent = ua.random\n",
        "options.add_argument(f'user-agent={user_agent}')\n",
        "\n",
        "# Run Chrome in headless mode to avoid graphical interface issues\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--disable-gpu\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "\n",
        "# Start ChromeDriver with options\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "# Load the root URL\n",
        "driver.get(root_url)\n",
        "\n",
        "# Create an empty list to store article URLs\n",
        "article_urls = []\n",
        "unique_urls = set()\n",
        "\n",
        "# Set the target number of articles\n",
        "target_articles = 5000\n",
        "\n",
        "# Load more until the target number of articles is reached or no more articles to load\n",
        "while len(unique_urls) < target_articles:\n",
        "    try:\n",
        "        # Extract URLs from the current page\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        articles = soup.select('article')\n",
        "\n",
        "        for article in articles:\n",
        "            try:\n",
        "                article_info = extract_article_data(article)\n",
        "                article_hash = hashlib.sha256(json.dumps(article_info, sort_keys=True).encode()).hexdigest()\n",
        "                if article_hash not in unique_urls:\n",
        "                    unique_urls.add(article_hash)\n",
        "                    article_urls.append(article_info)\n",
        "                    print(\"URL '{}' has been processed\".format(article_info['URL']))\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while processing an article: {str(e)}\")\n",
        "\n",
        "        # Execute JavaScript to click the 'Load More' button\n",
        "        load_more_button = driver.find_elements(By.XPATH, \"//button[contains(@class, 'ListingSide_btn') and text()='Load More']\")\n",
        "        if load_more_button:\n",
        "            driver.execute_script(\"arguments[0].click();\", load_more_button[0])\n",
        "            scroll_to_bottom(driver)\n",
        "        else:\n",
        "            print(\"No more articles to load.\")\n",
        "            break\n",
        "    except StaleElementReferenceException:\n",
        "        print(\"StaleElementReferenceException. Retrying...\")\n",
        "        time.sleep(2)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        break\n",
        "\n",
        "# Close the WebDriver\n",
        "driver.quit()\n",
        "\n",
        "# Display the number of unique URLs\n",
        "num_unique_urls = len(article_urls)\n",
        "print(f\"Number of unique URLs: {num_unique_urls}\")\n",
        "\n",
        "# Write the article URLs to a CSV file\n",
        "output_file_path = \"article_urls 1300.csv\"\n",
        "with open(output_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['URL']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for article_info in article_urls:\n",
        "        writer.writerow(article_info)\n",
        "\n"
      ],
      "metadata": {
        "id": "yLlF_BcrUIi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing url that contain, \"sach-ke-saathi\""
      ],
      "metadata": {
        "id": "Fg2rnq3NOKXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "def filter_urls(csv_file, column_index):\n",
        "    filtered_urls = []\n",
        "    with open(csv_file, 'r', newline='') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            url = row[column_index]\n",
        "            if not re.search(r'sach-', url):\n",
        "                filtered_urls.append(row)\n",
        "    return filtered_urls\n",
        "\n",
        "def write_filtered_csv(filtered_data, output_file):\n",
        "    with open(output_file, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerows(filtered_data)\n",
        "\n",
        "# Replace 'input.csv' and 'output.csv' with your file paths\n",
        "input_csv = \"/content/article_urls 1300.csv\"  # Your input CSV file\n",
        "output_csv = \"output 1293.csv\"  # Output CSV file after filtering\n",
        "url_column_index = 0  # Index of the column containing URLs\n",
        "\n",
        "filtered_data = filter_urls(input_csv, url_column_index)\n",
        "write_filtered_csv(filtered_data, output_csv)\n",
        "\n",
        "print(\"Done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5GsXxPL1QYZ",
        "outputId": "4e1a4f05-7761-4202-9fa1-1fecd9855c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding 'Extract URL' column to retrieve the full news from Vishwasnews."
      ],
      "metadata": {
        "id": "RNqPnzDVDEdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def extract_particular_links(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            particular_links = [link.get('href') for link in soup.find_all('a') if link.get('href') and link.get('href').startswith('https://www.vishvasnews.com/')]\n",
        "            return particular_links\n",
        "        else:\n",
        "            print(f\"Failed to fetch webpage {url}. Status code: {response.status_code}\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while processing webpage {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def process_csv(input_csv_file, output_csv_file):\n",
        "    total_links = 0\n",
        "    unique_links = set()\n",
        "\n",
        "    # Read input CSV file\n",
        "    df = pd.read_csv(input_csv_file)\n",
        "\n",
        "    # Extract URLs from 'Article_URL' column\n",
        "    urls = df['Article_URL'].tolist()\n",
        "\n",
        "    # Extract particular links for each URL using multithreading\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        results = executor.map(extract_particular_links, urls)\n",
        "\n",
        "    # Collect all unique links\n",
        "    for extracted_links in results:\n",
        "        for link in extracted_links:\n",
        "            unique_links.add(link)\n",
        "\n",
        "    # Write original columns along with the extracted URLs to the output CSV file\n",
        "    with open(output_csv_file, 'w', newline='') as output_file:\n",
        "        writer = csv.writer(output_file)\n",
        "        writer.writerow(df.columns.tolist() + [\"Extracted_URL\"])  # Write column headers\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            article_url = row['Article_URL']\n",
        "            extracted_links = extract_particular_links(article_url)\n",
        "            for link in extracted_links:\n",
        "                if link in unique_links:\n",
        "                    writer.writerow(row.tolist() + [link])\n",
        "                    unique_links.remove(link)\n",
        "                    total_links += 1\n",
        "\n",
        "    print(f\"Total number of extracted unique links: {total_links}\")\n",
        "\n",
        "# Provide the path to your input CSV file containing the URLs and the desired output CSV file\n",
        "input_csv_file = '/content/downloaded_images 1293.csv'\n",
        "output_csv_file = 'output_data New Not Repeat 2.csv'\n",
        "\n",
        "# Run the function to process CSV and extract URLs\n",
        "process_csv(input_csv_file, output_csv_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghRnTTNa9l3t",
        "outputId": "e90282bf-b843-42c6-9c3a-7e84c80c1926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of extracted unique links: 1138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run after the above one, so that as it adds the column \"Text under 'Fact Check'\" to it"
      ],
      "metadata": {
        "id": "cjMNu7ATDQmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def extract_content_from_url(row):\n",
        "    url = row['Extracted_URL']\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            claim_review_div = soup.find('div', class_='reviews')\n",
        "            if claim_review_div:\n",
        "                claim_review_ul = claim_review_div.find('ul', class_='claim-review')\n",
        "                if claim_review_ul:\n",
        "                    extracted_texts = []\n",
        "                    for li in claim_review_ul.find_all('li'):\n",
        "                        strong = li.find('strong', text=\"Fact Check :\")\n",
        "                        if strong:\n",
        "                            span = li.find('span')\n",
        "                            if span:\n",
        "                                extracted_texts.append(span.text.strip())\n",
        "                    row['Text under Fact Check'] = ', '.join(extracted_texts) if extracted_texts else \"No text found under 'Fact Check'\"\n",
        "                    return row\n",
        "            row['Text under Fact Check'] = \"No 'Fact Check' section found\"\n",
        "            return row\n",
        "        else:\n",
        "            print(f\"Failed to fetch webpage {url}. Status code: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while processing webpage {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_csv(input_csv_file, output_csv_file):\n",
        "    with open(input_csv_file, 'r', newline='') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        rows = [row for row in reader]\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        results = executor.map(extract_content_from_url, rows)\n",
        "\n",
        "    with open(output_csv_file, 'w', newline='') as output_file:\n",
        "        fieldnames = list(rows[0].keys()) + ['Text under Fact Check'] if 'Text under Fact Check' not in rows[0].keys() else list(rows[0].keys())\n",
        "        writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for result in results:\n",
        "            if result:\n",
        "                writer.writerow(result)\n",
        "\n",
        "# Provide the path to your CSV file containing the URLs and the desired output CSV file\n",
        "input_csv_file = '/content/output_data New Not Repeat 2.csv'\n",
        "output_csv_file = 'output Text Fact Type.csv'\n",
        "\n",
        "# Run the function to process CSV and extract content\n",
        "process_csv(input_csv_file, output_csv_file)\n"
      ],
      "metadata": {
        "id": "BlCIfQm2DVpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Title to above csv"
      ],
      "metadata": {
        "id": "q6_ctBKFLicO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def extract_title_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            title_tag = soup.find('h1', class_='article-heading')\n",
        "            if title_tag:\n",
        "                return title_tag.text.strip()\n",
        "            else:\n",
        "                return \"Title not found\"\n",
        "        else:\n",
        "            print(f\"Failed to fetch webpage {url}. Status code: {response.status_code}\")\n",
        "            return \"Title not found\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while processing webpage {url}: {e}\")\n",
        "        return \"Title not found\"\n",
        "\n",
        "def process_csv(input_csv_file, output_csv_file):\n",
        "    with open(input_csv_file, 'r', newline='') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        rows = [row for row in reader]\n",
        "\n",
        "    titles = []\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        results = executor.map(lambda row: extract_title_from_url(row['Extracted_URL']), rows)\n",
        "        titles.extend(results)\n",
        "\n",
        "    for i, row in enumerate(rows):\n",
        "        row['Title'] = titles[i]\n",
        "\n",
        "    with open(output_csv_file, 'w', newline='') as output_file:\n",
        "        fieldnames = list(rows[0].keys())\n",
        "        writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for row in rows:\n",
        "            writer.writerow(row)\n",
        "\n",
        "# Provide the path to your CSV file containing the URLs and the desired output CSV file\n",
        "input_csv_file = '/content/output Text Fact Type.csv'\n",
        "output_csv_file = 'output_file_with_title.csv'\n",
        "\n",
        "# Run the function to process CSV and extract content\n",
        "process_csv(input_csv_file, output_csv_file)\n"
      ],
      "metadata": {
        "id": "gDIPCMYRLiEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Description"
      ],
      "metadata": {
        "id": "IEzBJHt2RzRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def extract_description_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            main_article_div = soup.find('div', class_='main_article_area')\n",
        "            if main_article_div:\n",
        "                description_paragraphs = main_article_div.find_all('p')\n",
        "                description_text = ' '.join([p.text.strip() for p in description_paragraphs])\n",
        "                return description_text.strip()\n",
        "            else:\n",
        "                return \"Description not found\"\n",
        "        else:\n",
        "            print(f\"Failed to fetch webpage {url}. Status code: {response.status_code}\")\n",
        "            return \"Description not found\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while processing webpage {url}: {e}\")\n",
        "        return \"Description not found\"\n",
        "\n",
        "def process_csv(input_csv_file, output_csv_file):\n",
        "    with open(input_csv_file, 'r', newline='') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        fieldnames = reader.fieldnames + ['Description'] if 'Description' not in reader.fieldnames else reader.fieldnames\n",
        "        rows = [row for row in reader]\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        for row in rows:\n",
        "            url = row['Extracted_URL']\n",
        "            description = extract_description_from_url(url)\n",
        "            row['Description'] = description\n",
        "\n",
        "    with open(output_csv_file, 'w', newline='') as output_file:\n",
        "        writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for row in rows:\n",
        "            writer.writerow(row)\n",
        "\n",
        "# Provide the path to your CSV file containing the URLs and the desired output CSV file\n",
        "input_csv_file = '/content/output_file_with_title.csv'\n",
        "output_csv_file = 'output_file_with_description.csv'\n",
        "\n",
        "# Run the function to process CSV and extract content\n",
        "process_csv(input_csv_file, output_csv_file)\n"
      ],
      "metadata": {
        "id": "vaM8-52dMiMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping rows that contain :\"No 'Fact Check' section found\"]"
      ],
      "metadata": {
        "id": "tKaJDVEDaoxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(\"/output_file_with_description N.csv\")\n",
        "\n",
        "# Filter out rows where the \"Text under Fact Check\" column contains \"No 'Fact Check' section found\"\n",
        "df_filtered = df[df['Text under Fact Check'] != \"No 'Fact Check' section found\"]\n",
        "\n",
        "# Save the filtered DataFrame to a new CSV file\n",
        "df_filtered.to_csv(\"filtered_file.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "1GF8l69xl9M1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}